{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 275\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Scrape materials for Scandium (Sc)\u001b[39;00m\n\u001b[1;32m    274\u001b[0m element \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 275\u001b[0m materials \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_element_materials\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_details\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materials:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     scraper\u001b[38;5;241m.\u001b[39msave_to_csv(materials, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00melement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_materials.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 227\u001b[0m, in \u001b[0;36mTopologicalQuantumChemistryScraper.scrape_element_materials\u001b[0;34m(self, element_symbol, include_details)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_driver()\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Search for the element\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_by_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement_symbol\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_materials\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Scrape search results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m, in \u001b[0;36mTopologicalQuantumChemistryScraper.search_by_element\u001b[0;34m(self, element_symbol)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Find and click on the element in the periodic table\u001b[39;00m\n\u001b[1;32m     61\u001b[0m element_xpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//div[contains(@class, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melement\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) or contains(@class, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)]//text()[normalize-space()=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00melement_symbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]/parent::*\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 62\u001b[0m element_button \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_clickable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement_xpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m element_button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Wait for search results to load\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m, in \u001b[0;36mTopologicalQuantumChemistryScraper.wait_for_clickable\u001b[0;34m(self, locator, timeout)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_for_clickable\u001b[39m(\u001b[38;5;28mself\u001b[39m, locator, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wait for an element to be clickable\"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_to_be_clickable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:137\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# there is no api :(\n",
    "# build webscraper.. ?\n",
    "# also analyze https://cryst.ehu.es/#solidtop for further classification recommendations\n",
    "\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import re\n",
    "\n",
    "class TopologicalQuantumChemistryScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the scraper with Chrome webdriver\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        \n",
    "        self.driver = None\n",
    "        self.base_url = \"https://topologicalquantumchemistry.org/#/\"\n",
    "        \n",
    "    def start_driver(self):\n",
    "        \"\"\"Start the Chrome webdriver\"\"\"\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "    def close_driver(self):\n",
    "        \"\"\"Close the webdriver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            \n",
    "    def wait_for_element(self, locator, timeout=10):\n",
    "        \"\"\"Wait for an element to be present\"\"\"\n",
    "        return WebDriverWait(self.driver, timeout).until(\n",
    "            EC.presence_of_element_located(locator)\n",
    "        )\n",
    "        \n",
    "    def wait_for_clickable(self, locator, timeout=10):\n",
    "        \"\"\"Wait for an element to be clickable\"\"\"\n",
    "        return WebDriverWait(self.driver, timeout).until(\n",
    "            EC.element_to_be_clickable(locator)\n",
    "        )\n",
    "        \n",
    "    def search_by_element(self, element_symbol):\n",
    "        \"\"\"Search for materials containing a specific element\"\"\"\n",
    "        try:\n",
    "            # Navigate to the main page\n",
    "            self.driver.get(self.base_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Find and click on the element in the periodic table\n",
    "            element_xpath = f\"//div[contains(@class, 'element') or contains(@class, 'cell')]//text()[normalize-space()='{element_symbol}']/parent::*\"\n",
    "            element_button = self.wait_for_clickable((By.XPATH, element_xpath))\n",
    "            element_button.click()\n",
    "            \n",
    "            # Wait for search results to load\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Click search button if it exists\n",
    "            try:\n",
    "                search_button = self.wait_for_clickable((By.XPATH, \"//button[contains(text(), 'Search') or contains(@class, 'search')]\"))\n",
    "                search_button.click()\n",
    "                time.sleep(3)\n",
    "            except TimeoutException:\n",
    "                print(\"Search button not found, proceeding...\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for element {element_symbol}: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def scrape_search_results(self):\n",
    "        \"\"\"Scrape the search results page for materials\"\"\"\n",
    "        materials = []\n",
    "        \n",
    "        try:\n",
    "            # Wait for results to load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Find all material entries in the results\n",
    "            # Look for common patterns in the results table/list\n",
    "            material_rows = self.driver.find_elements(By.XPATH, \"//tr[contains(@class, 'material') or contains(@class, 'entry') or td]\")\n",
    "            \n",
    "            if not material_rows:\n",
    "                # Try alternative selectors\n",
    "                material_rows = self.driver.find_elements(By.XPATH, \"//div[contains(@class, 'result') or contains(@class, 'material')]\")\n",
    "            \n",
    "            print(f\"Found {len(material_rows)} material entries\")\n",
    "            \n",
    "            for row in material_rows:\n",
    "                try:\n",
    "                    material_data = {}\n",
    "                    \n",
    "                    # Extract compound name\n",
    "                    compound_elem = row.find_elements(By.XPATH, \".//td[1] | .//div[1] | .//*[contains(@class, 'compound')]\")\n",
    "                    if compound_elem:\n",
    "                        material_data['compound'] = compound_elem[0].text.strip()\n",
    "                    \n",
    "                    # Extract symmetry group\n",
    "                    symmetry_elem = row.find_elements(By.XPATH, \".//td[2] | .//*[contains(@class, 'symmetry')]\")\n",
    "                    if symmetry_elem:\n",
    "                        material_data['symmetry_group'] = symmetry_elem[0].text.strip()\n",
    "                    \n",
    "                    # Extract topological indices\n",
    "                    topo_elem = row.find_elements(By.XPATH, \".//td[3] | .//*[contains(@class, 'topological')]\")\n",
    "                    if topo_elem:\n",
    "                        material_data['topological_indices'] = topo_elem[0].text.strip()\n",
    "                    \n",
    "                    # Extract crossing type\n",
    "                    crossing_elem = row.find_elements(By.XPATH, \".//td[4] | .//*[contains(@class, 'crossing')]\")\n",
    "                    if crossing_elem:\n",
    "                        material_data['crossing_type'] = crossing_elem[0].text.strip()\n",
    "                    \n",
    "                    # Extract material type\n",
    "                    type_elem = row.find_elements(By.XPATH, \".//td[5] | .//*[contains(@class, 'type')]\")\n",
    "                    if type_elem:\n",
    "                        material_data['type'] = type_elem[0].text.strip()\n",
    "                    \n",
    "                    # Look for clickable links to detail pages\n",
    "                    detail_links = row.find_elements(By.XPATH, \".//a[contains(@href, 'detail') or contains(@href, '#')]\")\n",
    "                    if detail_links:\n",
    "                        href = detail_links[0].get_attribute('href')\n",
    "                        material_data['detail_url'] = href\n",
    "                        \n",
    "                        # Extract ICSD number from URL if present\n",
    "                        if 'detail' in href:\n",
    "                            icsd_match = re.search(r'detail/(\\d+)', href)\n",
    "                            if icsd_match:\n",
    "                                material_data['icsd_number'] = icsd_match.group(1)\n",
    "                    \n",
    "                    if material_data and ('compound' in material_data or len(material_data) > 1):\n",
    "                        materials.append(material_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting data from row: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping search results: {str(e)}\")\n",
    "            \n",
    "        return materials\n",
    "        \n",
    "    def scrape_material_details(self, detail_url):\n",
    "        \"\"\"Scrape detailed information for a specific material\"\"\"\n",
    "        try:\n",
    "            self.driver.get(detail_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            material_details = {}\n",
    "            \n",
    "            # Extract basic information\n",
    "            compound_elem = self.driver.find_elements(By.XPATH, \"//*[contains(text(), 'Compound')]/following-sibling::* | //*[contains(@class, 'compound')]\")\n",
    "            if compound_elem:\n",
    "                material_details['compound'] = compound_elem[0].text.strip()\n",
    "            \n",
    "            # Extract crystallographic data\n",
    "            try:\n",
    "                # Cell parameters\n",
    "                cell_length_a = self.driver.find_elements(By.XPATH, \"//*[contains(text(), 'Cell Length A')]/following-sibling::* | //*[contains(text(), 'a =')]\")\n",
    "                if cell_length_a:\n",
    "                    material_details['cell_length_a'] = cell_length_a[0].text.strip()\n",
    "                    \n",
    "                cell_length_b = self.driver.find_elements(By.XPATH, \"//*[contains(text(), 'Cell Length B')]/following-sibling::* | //*[contains(text(), 'b =')]\")\n",
    "                if cell_length_b:\n",
    "                    material_details['cell_length_b'] = cell_length_b[0].text.strip()\n",
    "                    \n",
    "                cell_length_c = self.driver.find_elements(By.XPATH, \"//*[contains(text(), 'Cell Length C')]/following-sibling::* | //*[contains(text(), 'c =')]\")\n",
    "                if cell_length_c:\n",
    "                    material_details['cell_length_c'] = cell_length_c[0].text.strip()\n",
    "                    \n",
    "                # Cell angles\n",
    "                cell_angles = self.driver.find_elements(By.XPATH, \"//*[contains(text(), 'Cell Angle')]/following-sibling::*\")\n",
    "                for angle in cell_angles:\n",
    "                    text = angle.text.strip()\n",
    "                    if 'α' in text or 'alpha' in text.lower():\n",
    "                        material_details['cell_angle_alpha'] = text\n",
    "                    elif 'β' in text or 'beta' in text.lower():\n",
    "                        material_details['cell_angle_beta'] = text\n",
    "                    elif 'γ' in text or 'gamma' in text.lower():\n",
    "                        material_details['cell_angle_gamma'] = text\n",
    "                        \n",
    "                # Cell volume\n",
    "                cell_volume = self.driver.find_elements(By.XPATH, \"//*[contains(text(), 'Cell Volume')]/following-sibling::*\")\n",
    "                if cell_volume:\n",
    "                    material_details['cell_volume'] = cell_volume[0].text.strip()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting crystallographic data: {str(e)}\")\n",
    "                \n",
    "            # Extract topological information\n",
    "            try:\n",
    "                symmetry_group = self.driver.find_elements(By.XPATH, \"//*[contains(text(), 'Symmetry Group')]/following-sibling::*\")\n",
    "                if symmetry_group:\n",
    "                    material_details['symmetry_group'] = symmetry_group[0].text.strip()\n",
    "                    \n",
    "                topological_indices = self.driver.find_elements(By.XPATH, \"//*[contains(text(), 'Topological Indices')]/following-sibling::*\")\n",
    "                if topological_indices:\n",
    "                    material_details['topological_indices'] = topological_indices[0].text.strip()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting topological data: {str(e)}\")\n",
    "                \n",
    "            return material_details\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping material details from {detail_url}: {str(e)}\")\n",
    "            return {}\n",
    "            \n",
    "    def scrape_element_materials(self, element_symbol, include_details=False):\n",
    "        \"\"\"Main method to scrape all materials for a given element\"\"\"\n",
    "        all_materials = []\n",
    "        \n",
    "        try:\n",
    "            self.start_driver()\n",
    "            \n",
    "            # Search for the element\n",
    "            if not self.search_by_element(element_symbol):\n",
    "                return all_materials\n",
    "                \n",
    "            # Scrape search results\n",
    "            materials = self.scrape_search_results()\n",
    "            print(f\"Found {len(materials)} materials for element {element_symbol}\")\n",
    "            \n",
    "            # Optionally scrape detailed information\n",
    "            if include_details:\n",
    "                for i, material in enumerate(materials):\n",
    "                    if 'detail_url' in material:\n",
    "                        print(f\"Scraping details for material {i+1}/{len(materials)}\")\n",
    "                        details = self.scrape_material_details(material['detail_url'])\n",
    "                        material.update(details)\n",
    "                        time.sleep(1)  # Be respectful to the server\n",
    "                        \n",
    "            all_materials = materials\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in scrape_element_materials: {str(e)}\")\n",
    "            \n",
    "        finally:\n",
    "            self.close_driver()\n",
    "            \n",
    "        return all_materials\n",
    "        \n",
    "    def save_to_csv(self, materials, filename):\n",
    "        \"\"\"Save materials data to CSV file\"\"\"\n",
    "        if not materials:\n",
    "            print(\"No materials to save\")\n",
    "            return\n",
    "            \n",
    "        df = pd.DataFrame(materials)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(materials)} materials to {filename}\")\n",
    "        \n",
    "    def save_to_json(self, materials, filename):\n",
    "        \"\"\"Save materials data to JSON file\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(materials, f, indent=2)\n",
    "        print(f\"Saved {len(materials)} materials to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = TopologicalQuantumChemistryScraper(headless=False)  # Set to True for headless mode\n",
    "    \n",
    "    # Scrape materials for Scandium (Sc)\n",
    "    element = \"Sc\"\n",
    "    materials = scraper.scrape_element_materials(element, include_details=True)\n",
    "    \n",
    "    if materials:\n",
    "        # Save results\n",
    "        scraper.save_to_csv(materials, f\"{element}_materials.csv\")\n",
    "        scraper.save_to_json(materials, f\"{element}_materials.json\")\n",
    "        \n",
    "        print(f\"\\nSample material data:\")\n",
    "        for material in materials[:3]:  # Show first 3 materials\n",
    "            print(json.dumps(material, indent=2))\n",
    "    else:\n",
    "        print(f\"No materials found for element {element}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
